{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17776,
     "status": "ok",
     "timestamp": 1728026996326,
     "user": {
      "displayName": "Ruth Parajó Ferrer",
      "userId": "07031545869212809064"
     },
     "user_tz": -120
    },
    "id": "xJJ2jzpfkpt0",
    "outputId": "435323a9-257c-4d2a-9b4e-33146751fa24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 09:20:32.074365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 09:20:32.094658: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 09:20:32.100802: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 09:20:32.116334: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 09:20:33.289490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1728026997084,
     "user": {
      "displayName": "Ruth Parajó Ferrer",
      "userId": "07031545869212809064"
     },
     "user_tz": -120
    },
    "id": "cdgXKy8WkrF0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibqF7sdakyQ6"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lst : (147, 256, 256, 3)\n",
      "wt : (147, 256, 256)\n",
      "ndvi : (147, 256, 256)\n",
      "slope : (147, 256, 256)\n",
      "discharge : (147, 256, 256)\n",
      "masked : (147, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Set variables\n",
    "source_folder = '../data/external/raster_masks'\n",
    "rivers = {}\n",
    "source_path = '../data/preprocessed/'\n",
    "data_paths = ['lst','wt','ndvi','slope', 'discharge','masked']#,'slope', 'discharge']#, 'ndvi', 'wt', 'masked','discharge', 'slope']#, 'wt_interpolated']\n",
    "dir_paths = [os.path.join(source_path,p) for p in data_paths]\n",
    "all_dir_paths = {k:[] for k in data_paths}    \n",
    "total_data = {}\n",
    "total_times = {}\n",
    "complete_rivers = []\n",
    "filter_river = None\n",
    "W=256\n",
    "\n",
    "# Load rivers\n",
    "for subdir, dirs, files in os.walk(source_folder):\n",
    "    for i,file in enumerate(files):\n",
    "        r,m = load_raster(os.path.join(subdir, file), False)\n",
    "        name = file.split('.')[0].split('bw_')[-1]\n",
    "        rivers[name] = r\n",
    "\n",
    "# Load input paths\n",
    "for i,dir_p in enumerate(dir_paths):\n",
    "    for subdir, dirs,files in os.walk(dir_p):\n",
    "        if subdir != dir_p and not subdir.endswith('masked') and not subdir.endswith('.ipynb_checkpoints'): \n",
    "            all_dir_paths[data_paths[i]].append(subdir)\n",
    "        elif subdir.endswith('masked'):\n",
    "            all_dir_paths['masked'].append(subdir)\n",
    "\n",
    "# Load input data\n",
    "for k,v in all_dir_paths.items():\n",
    "    if filter_river != None:\n",
    "        v = [v[i] for i in filter_river]\n",
    "    \n",
    "    if k != 'discharge' and k != 'slope':\n",
    "        if k == 'lst' or k == 'masked':\n",
    "            list_rgb = [True]*len(v)\n",
    "        else:\n",
    "            list_rgb = [False]*len(v)\n",
    "            \n",
    "        data, times = load_data(v,W,list_rgb)\n",
    "        if k!='masked':\n",
    "            labels = []\n",
    "            for ki,value in data.items():\n",
    "                labels+=[ki.split('/')[-1]]*len(value)\n",
    "        \n",
    "        filtered = [arr for arr in data.values() if arr.size > 0]\n",
    "\n",
    "        total_data[k] = np.concatenate(filtered, axis=0)\n",
    "        total_times[k] = times\n",
    "        print(k,':' ,total_data[k].shape)\n",
    "\n",
    "    elif k == 'discharge' or k == 'slope':\n",
    "        total = []\n",
    "        for p in v:\n",
    "            for file in os.listdir(p):\n",
    "                file_path = os.path.join(p, file)\n",
    "                r,m = load_raster(file_path, False)\n",
    "                var = resize_image(r, W,W)\n",
    "                img_river = labels.count(p.split(\"/\")[-1])\n",
    "                var_input = np.tile(var, (img_river, 1, 1))\n",
    "                total.append(var_input)\n",
    "        \n",
    "        total_data[k] = np.concatenate(total, axis=0)\n",
    "        print(k,':' ,total_data[k].shape)\n",
    "\n",
    "# Hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "river_encoded = encoder.fit_transform(np.array(labels).reshape(-1, 1))\n",
    "data_targets = total_data['wt']\n",
    "results = {'MAE':0,'MSE':0,'RMSE':0,'R²':0,'MAPE (%)':0,'MSE sample-wise':0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y2TXyInRBmC"
   },
   "source": [
    "## Do experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 256\n",
    "filter_river = None#[3,11,12]\n",
    "inputs = ['lst','ndvi','discharge', 'slope']#['ndvi','discharge', 'slope']\n",
    "conditioned = True\n",
    "batch_size = 8\n",
    "epochs = 300\n",
    "model_name = \"img_wise_CNN\" #img_wise_CNN, UNet, transfer_learning_VGG16, CNN, img_2_img\n",
    "stratified = False\n",
    "physics_guided = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(test_target, test_prediction, rivers, labels, test_index):\n",
    "    mean_results = {k:[] for k in results.keys()}\n",
    "    # Loop through each sample and compute the MSE for that sample\n",
    "    for i in range(test_target.shape[0]):\n",
    "        # Flatten the true and predicted values for this sample\n",
    "        riv = rivers[labels[test_index[i]]].flatten()\n",
    "        y_true_flatten = test_target[i].flatten()\n",
    "        y_true_mask = y_true_flatten[riv != 0]\n",
    "        y_pred_flatten = test_prediction[i].flatten()\n",
    "        y_pred_mask = y_pred_flatten[riv != 0]\n",
    "        # Calculate metrics\n",
    "        res = evaluate_model(y_true_mask, y_pred_mask)\n",
    "        for k,v in res.items():\n",
    "            mean_results[k].append(v)\n",
    "    for key in mean_results:\n",
    "        mean_results[key] = np.mean(mean_results[key])\n",
    "    return mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose inputs\n",
    "inputs_d = [total_data[inp] for inp in inputs]\n",
    "# List to store the processed additional images\n",
    "expanded_images = []\n",
    "# Expand dimensions for single-channel images, leave multi-channel images as they are\n",
    "for img in inputs_d:\n",
    "    if img.ndim == 3:  # Case where image is (n, 256, 256) (single-channel)\n",
    "        expanded_images.append(np.expand_dims(img, axis=-1))  # Expand to add an extra channel\n",
    "    elif img.ndim == 4:  # Case where image already has multiple channels (n, 256, 256, c)\n",
    "        expanded_images.append(img)  # Leave the image as it is\n",
    "# Concatenate all images along the last axis (channels)\n",
    "combined_input = np.concatenate(expanded_images, axis=-1)\n",
    "# The final combined input is stored in input_data\n",
    "input_data = combined_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 88 imágenes, (88, 256, 256, 6)\n",
      "Val: 29 imágenes, (29, 256, 256, 6)\n",
      "Test: 30 imágenes, (30, 256, 256, 6)\n",
      "(88, 256, 256) (29, 256, 256) (30, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "time_split = True\n",
    "if time_split:\n",
    "    train_ratio = 0.6\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "    \n",
    "    # Calcular el tamaño de cada conjunto\n",
    "    total_images = len(input_data)\n",
    "    train_size = int(total_images * train_ratio)\n",
    "    val_size = int(total_images * val_ratio)\n",
    "    indices = np.arange(total_images)\n",
    "    \n",
    "    train_index = indices[:train_size]                       # Primeros índices para entrenamiento\n",
    "    validation_index = indices[train_size:train_size + val_size]    # Siguientes índices para validación\n",
    "    test_index = indices[train_size + val_size:]             # Últimos índices para prueba\n",
    "   \n",
    "elif stratified:\n",
    "    train_index, validation_index, test_index = split_data_stratified(input_data, data_targets, labels)\n",
    "else:\n",
    "    train_index, validation_index, test_index = split_data(input_data, data_targets)\n",
    "        \n",
    "validation_input = input_data[validation_index, :] / 255.0  # Normalize inputs\n",
    "validation_target = data_targets[validation_index, :]\n",
    "validation_rivers = river_encoded[validation_index, :]\n",
    "test_input = input_data[test_index, :] / 255.0  # Normalize inputs\n",
    "test_target = data_targets[test_index, :]\n",
    "test_rivers = river_encoded[test_index, :]\n",
    "train_input = input_data[train_index, :] / 255.0  # Normalize inputs\n",
    "train_target = data_targets[train_index, :]\n",
    "train_rivers = river_encoded[train_index, :]\n",
    "print(f\"Train: {len(train_input)} imágenes, {train_input.shape}\")\n",
    "print(f\"Val: {len(validation_input)} imágenes, {validation_input.shape}\")\n",
    "print(f\"Test: {len(test_input)} imágenes, {test_input.shape}\")\n",
    "print(train_target.shape, validation_target.shape, test_target.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with model=img_wise_CNN, batch_size=8, epochs=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 09:20:51.194210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 103 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-10-31 09:20:51.194846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 103 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1\n",
      "2024-10-31 09:21:02.376461: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 264.00MiB (rounded to 276824064)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-10-31 09:21:02.376509: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2024-10-31 09:21:02.376531: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 13, Chunks in use: 13. 3.2KiB allocated for chunks. 3.2KiB in use in bin. 492B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376548: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376564: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376579: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 3.5KiB allocated for chunks. 3.5KiB in use in bin. 3.4KiB client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376593: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 5.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376607: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376623: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 18.0KiB allocated for chunks. 18.0KiB in use in bin. 18.0KiB client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376638: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0. 36.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376651: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376664: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376680: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 256.0KiB allocated for chunks. 256.0KiB in use in bin. 256.0KiB client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376693: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376706: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376719: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376732: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376762: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 1. 59.82MiB allocated for chunks. 28.07MiB in use in bin. 16.00MiB client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376779: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 1. 43.24MiB allocated for chunks. 43.24MiB in use in bin. 30.03MiB client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376792: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376824: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-10-31 09:21:02.376839: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 264.00MiB was 256.00MiB, Chunk State: \n",
      "2024-10-31 09:21:02.376851: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 108396544\n",
      "2024-10-31 09:21:02.376867: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000000 of size 256 next 1\n",
      "2024-10-31 09:21:02.376879: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000100 of size 1280 next 2\n",
      "2024-10-31 09:21:02.376891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000600 of size 256 next 3\n",
      "2024-10-31 09:21:02.376902: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000700 of size 256 next 4\n",
      "2024-10-31 09:21:02.376913: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000800 of size 256 next 5\n",
      "2024-10-31 09:21:02.376924: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000900 of size 256 next 7\n",
      "2024-10-31 09:21:02.376935: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000a00 of size 256 next 8\n",
      "2024-10-31 09:21:02.376946: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000b00 of size 256 next 6\n",
      "2024-10-31 09:21:02.376957: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000c00 of size 256 next 9\n",
      "2024-10-31 09:21:02.376969: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000d00 of size 256 next 14\n",
      "2024-10-31 09:21:02.376980: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000e00 of size 256 next 12\n",
      "2024-10-31 09:21:02.376991: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780000f00 of size 256 next 13\n",
      "2024-10-31 09:21:02.377001: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780001000 of size 256 next 19\n",
      "2024-10-31 09:21:02.377012: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780001100 of size 256 next 17\n",
      "2024-10-31 09:21:02.377024: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f9780001200 of size 5632 next 10\n",
      "2024-10-31 09:21:02.377035: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780002800 of size 3584 next 11\n",
      "2024-10-31 09:21:02.377047: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f9780003600 of size 36864 next 16\n",
      "2024-10-31 09:21:02.377058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f978000c600 of size 18432 next 15\n",
      "2024-10-31 09:21:02.377070: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9780010e00 of size 262144 next 18\n",
      "2024-10-31 09:21:02.377081: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f9780050e00 of size 33292288 next 25\n",
      "2024-10-31 09:21:02.377092: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9782010e00 of size 29432320 next 21\n",
      "2024-10-31 09:21:02.377104: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9783c22800 of size 45340672 next 18446744073709551615\n",
      "2024-10-31 09:21:02.377115: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2024-10-31 09:21:02.377129: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 13 Chunks of size 256 totalling 3.2KiB\n",
      "2024-10-31 09:21:02.377142: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-10-31 09:21:02.377157: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3584 totalling 3.5KiB\n",
      "2024-10-31 09:21:02.377170: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 18432 totalling 18.0KiB\n",
      "2024-10-31 09:21:02.377185: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 262144 totalling 256.0KiB\n",
      "2024-10-31 09:21:02.377199: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 29432320 totalling 28.07MiB\n",
      "2024-10-31 09:21:02.377212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 45340672 totalling 43.24MiB\n",
      "2024-10-31 09:21:02.377225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 71.58MiB\n",
      "2024-10-31 09:21:02.377238: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 108396544 memory_limit_: 108396544 available bytes: 0 curr_region_allocation_bytes_: 216793088\n",
      "2024-10-31 09:21:02.377256: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                       108396544\n",
      "InUse:                        75061760\n",
      "MaxInUse:                    108354816\n",
      "NumAllocs:                          51\n",
      "MaxAllocSize:                 45340672\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-10-31 09:21:02.377271: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] *______________________________****************xxxxxxxxxxx******************************xxxxxxxxxxxx\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(model_input, train_target, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_data\u001b[38;5;241m=\u001b[39m(val_model_input, validation_target))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m     48\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:826\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "print(f\"Running experiment with model={model_name}, batch_size={batch_size}, epochs={epochs}\")\n",
    "\n",
    "if len(train_input.shape) == 3:\n",
    "    input_shape = train_input.shape[1:]+(1,)\n",
    "else:\n",
    "    input_shape = train_input.shape[1:]\n",
    "\n",
    "\n",
    "# Adapt input to condition\n",
    "if conditioned:\n",
    "    input_args = (input_shape, river_encoded.shape[1])\n",
    "    model_input = (train_input, train_rivers)\n",
    "    val_model_input = [validation_input, validation_rivers]\n",
    "    test_model_input = [test_input, test_rivers]\n",
    "else:\n",
    "    input_args = input_shape\n",
    "    model_input = train_input\n",
    "    val_model_input = validation_input\n",
    "    test_model_input = test_input\n",
    "\n",
    "\n",
    "# Start model\n",
    "start_time = time.time()\n",
    "if model_name == \"img_wise_CNN\":\n",
    "    if conditioned:\n",
    "        model = build_simplified_cnn_model_label(input_args[0], input_args[1])\n",
    "    else:\n",
    "        model = build_simplified_cnn_model(input_args)\n",
    "elif model_name == 'CNN':\n",
    "    model = build_cnn_model(input_args)\n",
    "elif model_name == 'img_2_img':\n",
    "    model = build_img_2_img_model(input_args)\n",
    "elif model_name == 'UNet':\n",
    "    model = build_unet(input_args)\n",
    "elif model_name == 'transfer_learning_VGG16':\n",
    "    train_input = train_input[:, :, :, :3]\n",
    "    model = build_transfer_model((W, W, 3))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "if not physics_guided:\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    history = model.fit(model_input, train_target, batch_size=batch_size, epochs=epochs, validation_data=(val_model_input, validation_target))\n",
    "else:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((*model_input, train_target) if isinstance(model_input, tuple) else (model_input, train_target))\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch in dataset:\n",
    "            # Handle batch based on whether model_input is a tuple or a single dataset\n",
    "            if isinstance(model_input, tuple):\n",
    "                model_input_batch = batch[:-1]  # All except the last element (target_batch)\n",
    "                target_batch = batch[-1]        # Last element is target_batch\n",
    "            else:\n",
    "                model_input_batch, target_batch = batch  # Direct unpacking for single dataset\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model([*model_input_batch], training=True) if isinstance(model_input_batch, tuple) else model(model_input_batch, training=True)\n",
    "                loss = conservation_energy_loss(target_batch, y_pred, model_input_batch, alpha=0.5, beta=0.5)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "# Evaluate results\n",
    "#validation_prediction = model.predict(val_model_input)\n",
    "test_prediction = model.predict(test_model_input)\n",
    "\n",
    "print('\\nComputing result metrics...')\n",
    "mean_results = get_results(test_target, test_prediction, rivers, labels, test_index)\n",
    "\n",
    "# Get experiment data\n",
    "end_time = time.time()\n",
    "duration = round(end_time - start_time, 2)\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "# Save model results\n",
    "laabeel = 'label' if conditioned else 'no label'\n",
    "var_inputs = '' if inputs == None else ', '.join(inputs)\n",
    "variables = ', '.join([var_inputs, laabeel,'stratified'])\n",
    "details = {'RMSE':mean_results['RMSE'],'Variables':variables,'Input': f'{len(np.unique(labels))} rivers', 'Output': 'wt', \\\n",
    "           'Resolution': W, 'nº samples': len(data_targets), 'Batch size': batch_size, 'Epochs': epochs, 'Date':current_date, \\\n",
    "           'Time':current_time, 'Duration': duration, 'Loss': 'Physics-guided'}\n",
    "\n",
    "file_path = f\"../results/{model_name}_results.xlsx\"\n",
    "save_excel(file_path, details, excel = 'Results')\n",
    "\n",
    "mean_results['Model'] = model_name\n",
    "file_path = f\"../results/all_results.xlsx\"\n",
    "save_excel(file_path, mean_results, excel = 'Results')\n",
    "\n",
    "print(f\"Experiment {model_name} with batch_size={batch_size} and epochs={epochs} completed.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Variables</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>nº samples</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.596295</td>\n",
       "      <td>lst, ndvi, no label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>16:53:12</td>\n",
       "      <td>64.91</td>\n",
       "      <td>Physics-guided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.613289</td>\n",
       "      <td>lst, ndvi, no label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>16:54:33</td>\n",
       "      <td>79.97</td>\n",
       "      <td>Physics-guided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.633750</td>\n",
       "      <td>lst, ndvi, no label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>16:56:19</td>\n",
       "      <td>106.00</td>\n",
       "      <td>Physics-guided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.595280</td>\n",
       "      <td>lst, ndvi, no label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>16:57:19</td>\n",
       "      <td>59.63</td>\n",
       "      <td>Physics-guided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.624425</td>\n",
       "      <td>lst, ndvi, no label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>16:58:37</td>\n",
       "      <td>76.82</td>\n",
       "      <td>Physics-guided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3.676263</td>\n",
       "      <td>lst, label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>10:29:27</td>\n",
       "      <td>65.06</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3.800815</td>\n",
       "      <td>lst, no label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>10:30:38</td>\n",
       "      <td>70.37</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3.782419</td>\n",
       "      <td>lst, label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>10:31:49</td>\n",
       "      <td>70.42</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3.952747</td>\n",
       "      <td>lst, no label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>10:33:08</td>\n",
       "      <td>78.60</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4.091063</td>\n",
       "      <td>lst, label</td>\n",
       "      <td>13 rivers</td>\n",
       "      <td>wt</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>10:34:27</td>\n",
       "      <td>78.93</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE            Variables      Input Output  Resolution  nº samples  \\\n",
       "0   3.596295  lst, ndvi, no label  13 rivers     wt         256         147   \n",
       "1   3.613289  lst, ndvi, no label  13 rivers     wt         256         147   \n",
       "2   3.633750  lst, ndvi, no label  13 rivers     wt         256         147   \n",
       "3   3.595280  lst, ndvi, no label  13 rivers     wt         256         147   \n",
       "4   3.624425  lst, ndvi, no label  13 rivers     wt         256         147   \n",
       "..       ...                  ...        ...    ...         ...         ...   \n",
       "69  3.676263           lst, label  13 rivers     wt         256         147   \n",
       "70  3.800815        lst, no label  13 rivers     wt         256         147   \n",
       "71  3.782419           lst, label  13 rivers     wt         256         147   \n",
       "72  3.952747        lst, no label  13 rivers     wt         256         147   \n",
       "73  4.091063           lst, label  13 rivers     wt         256         147   \n",
       "\n",
       "    Batch size  Epochs        Date      Time  Duration            Loss  \n",
       "0           16      10  2024-10-30  16:53:12     64.91  Physics-guided  \n",
       "1           16      50  2024-10-30  16:54:33     79.97  Physics-guided  \n",
       "2           16     100  2024-10-30  16:56:19    106.00  Physics-guided  \n",
       "3           32      10  2024-10-30  16:57:19     59.63  Physics-guided  \n",
       "4           32      50  2024-10-30  16:58:37     76.82  Physics-guided  \n",
       "..         ...     ...         ...       ...       ...             ...  \n",
       "69          32      10  2024-10-31  10:29:27     65.06            RMSE  \n",
       "70          32      50  2024-10-31  10:30:38     70.37            RMSE  \n",
       "71          32      50  2024-10-31  10:31:49     70.42            RMSE  \n",
       "72          32     100  2024-10-31  10:33:08     78.60            RMSE  \n",
       "73          32     100  2024-10-31  10:34:27     78.93            RMSE  \n",
       "\n",
       "[74 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model_name = 'img_wise_CNN'\n",
    "pd.read_excel(f'../results/{model_name}_results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1728047630422,
     "user": {
      "displayName": "Ruth Parajó Ferrer",
      "userId": "07031545869212809064"
     },
     "user_tz": -120
    },
    "id": "8lInUBSuqaJL",
    "outputId": "6da1d732-2f2a-4228-d35f-de06e5ff7803",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1728046187777,
     "user": {
      "displayName": "Ruth Parajó Ferrer",
      "userId": "07031545869212809064"
     },
     "user_tz": -120
    },
    "id": "EajYGsQBjG2G",
    "outputId": "1b005f05-2bee-4259-af17-0382a232a40c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "#plt.clf\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Simpler CNN MSE Loss during training --- lst+ndvi with wt target')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('../plots/cnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1728043130132,
     "user": {
      "displayName": "Ruth Parajó Ferrer",
      "userId": "07031545869212809064"
     },
     "user_tz": -120
    },
    "id": "0EYej4lojG2G",
    "outputId": "ea68cef6-398f-422a-8401-e6c02609ee06"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('MAE during training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv2k19g5jG2H"
   },
   "source": [
    "#### Validate and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow08ZqWsjG2J"
   },
   "source": [
    "Image wise metrics results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbJ77fRFjG2J"
   },
   "source": [
    "See what are the areas with more prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4054,
     "status": "ok",
     "timestamp": 1728043150628,
     "user": {
      "displayName": "Ruth Parajó Ferrer",
      "userId": "07031545869212809064"
     },
     "user_tz": -120
    },
    "id": "SqvaSLQhjtR6",
    "outputId": "4d3a25c8-67ba-434a-ef58-cf84d17905af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(validation_prediction.shape) == 4:\n",
    "  validation_prediction=np.squeeze(validation_prediction, axis=3)\n",
    "\n",
    "diff = validation_prediction - validation_target\n",
    "for i in range(diff.shape[0]):\n",
    "  sns.heatmap(diff[i], cmap='coolwarm')\n",
    "  plt.title('Prediction Error Heatmap')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42nOYweqjG2K"
   },
   "source": [
    "Dispersion graph and histogram of prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target_flat = validation_target.reshape(-1)\n",
    "validation_prediction_flat = validation_prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1728043151456,
     "user": {
      "displayName": "Ruth Parajó Ferrer",
      "userId": "07031545869212809064"
     },
     "user_tz": -120
    },
    "id": "AtO-_TdpjG2K",
    "outputId": "c077eb5e-f4e6-4156-f9f2-55468aa1b0fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Supongamos que y_true son tus valores reales y y_pred son tus predicciones\n",
    "y_true = validation_target_flat # Valores reales\n",
    "y_pred = validation_prediction_flat  # Predicciones del modelo\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(y_true, y_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()\n",
    "\n",
    "# Histograma de errores\n",
    "errors = y_pred - y_true\n",
    "plt.hist(errors, bins=30)\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Prediction Errors')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMOhs4aAZ7i01xBD4Eg2RuL",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
