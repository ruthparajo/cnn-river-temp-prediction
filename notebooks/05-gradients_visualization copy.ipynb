{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f241d1-150c-4f4a-a070-93338b3910af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentinelhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f7f84f-64f8-4f8f-a6eb-850ec767caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from rasterio.windows import Window\n",
    "from collections import Counter\n",
    "from contextlib import redirect_stdout\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc5dd0",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ecdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(model, test_inp_labels, test_additional_inp, river_masks):\n",
    "    \"\"\"\n",
    "    Analyzes model gradients with respect to images and calculates the impact of variables \n",
    "    both generally and specifically for rivers.\n",
    "\n",
    "    Args:\n",
    "        model: TensorFlow model used for predictions.\n",
    "        test_inp_labels: Dictionary with test images grouped by cell.\n",
    "        test_additional_inp: Dictionary with additional inputs grouped by cell.\n",
    "        river_masks: Dictionary with binary river masks for each cell.\n",
    "\n",
    "    Returns:\n",
    "        grad_maps: Gradient maps for each variable and cell.\n",
    "        variable_impact_general: General impact (magnitude and direction) per variable and cell.\n",
    "        variable_impact_river: River-specific impact (magnitude and direction) per variable and cell.\n",
    "        cell_grad_analysis: Summary of general impact per cell and variable.\n",
    "        river_grad_analysis: Summary of river-specific impact per cell and variable.\n",
    "    \"\"\"\n",
    "    # Initialize data structures\n",
    "    grad_maps = {var: {} for var in [\"LST\", \"NDVI\", \"Slope\", \"Direction\", \"Altitude\"]}\n",
    "    variable_impact_general = {var: {} for var in grad_maps}\n",
    "    variable_impact_river = {var: {} for var in grad_maps}\n",
    "    cell_grad_analysis = {}\n",
    "    river_grad_analysis = {}\n",
    "\n",
    "    # Iterate through cells and test images\n",
    "    for cell, test_images in test_inp_labels.items():\n",
    "        cell_grad_analysis[cell] = {}\n",
    "        river_grad_analysis[cell] = {}\n",
    "\n",
    "        # Expand binary river mask\n",
    "        river_mask = np.pad(river_masks[cell], pad_width=1, mode='constant')\n",
    "        expanded_mask = (river_mask[:-2, :-2] | river_mask[1:-1, :-2] | river_mask[:-2, 1:-1] | \n",
    "                         river_mask[1:-1, 1:-1] | river_mask[:-2, 2:] | river_mask[1:-1, 2:])\n",
    "\n",
    "        for i, test_image in enumerate(test_images):\n",
    "            # Convert the image and additional inputs to tensors\n",
    "            test_image_tensor = tf.expand_dims(tf.convert_to_tensor(test_image, dtype=tf.float32), axis=0)\n",
    "            add_input = tf.expand_dims(tf.convert_to_tensor(test_additional_inp[cell][i], dtype=tf.float32), axis=0)\n",
    "\n",
    "            # Compute gradients using GradientTape\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(test_image_tensor)\n",
    "                preds = model([test_image_tensor, add_input], training=False)\n",
    "            gradients = tape.gradient(preds[:, 0], test_image_tensor).numpy()[0]\n",
    "\n",
    "            # Process gradients for each variable\n",
    "            for variable, channels in {\"LST\": range(3), \"NDVI\": [3], \"Slope\": [4], \"Direction\": [5], \"Altitude\": [6]}.items():\n",
    "                grads_map = gradients[:, :, channels].mean(axis=-1)  # Combine channels\n",
    "                grad_maps[variable].setdefault(cell, []).append(grads_map)\n",
    "\n",
    "        # Calculate impacts for each variable\n",
    "        for variable in grad_maps:\n",
    "            grad_images = np.array(grad_maps[variable][cell])\n",
    "            mean_grad = grad_images.mean(axis=0)\n",
    "\n",
    "            # General impacts\n",
    "            general_impact = np.abs(mean_grad).mean()\n",
    "            general_direction = np.sign(mean_grad).mean()\n",
    "\n",
    "            # River-specific impacts\n",
    "            river_impact = np.abs(mean_grad[expanded_mask]).mean()\n",
    "            river_direction = np.sign(mean_grad[expanded_mask]).mean()\n",
    "\n",
    "            # Store results\n",
    "            variable_impact_general[variable][cell] = (general_impact, general_direction)\n",
    "            variable_impact_river[variable][cell] = (river_impact, river_direction)\n",
    "\n",
    "            influence = \"increases\" if general_direction > 0 else \"decreases\"\n",
    "            cell_grad_analysis[cell][variable] = {'magnitude': general_impact, 'direction': general_direction}\n",
    "            river_grad_analysis[cell][variable] = {'magnitude': river_impact, 'direction': river_direction}\n",
    "\n",
    "    return grad_maps, variable_impact_general, variable_impact_river, cell_grad_analysis, river_grad_analysis\n",
    "\n",
    "\n",
    "\n",
    "def map_season(date):\n",
    "    \"\"\"Maps a date to a season based on the month.\"\"\"\n",
    "    month = pd.to_datetime(date).month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5, 9, 10, 11]:\n",
    "        return 'Spring-Autumn'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "\n",
    "def analyze_gradients_by_season(grad_folder, test_inp_labels, river_masks_cut, W=64):\n",
    "    \"\"\"\n",
    "    Analyzes gradients by cell and season, calculating mean magnitude and direction.\n",
    "    \"\"\"\n",
    "    seasonal_grad_analysis = {'Winter': {}, 'Spring-Autumn': {}, 'Summer': {}}\n",
    "\n",
    "    for cell, images_info in test_inp_labels.items():\n",
    "        cell_grad_folder = os.path.join(grad_folder, cell)\n",
    "        grad_images_by_season = {'Winter': [], 'Spring-Autumn': [], 'Summer': []}\n",
    "\n",
    "        for grad_file_name in os.listdir(cell_grad_folder):\n",
    "            grad_file_path = os.path.join(cell_grad_folder, grad_file_name)\n",
    "\n",
    "            if grad_file_name.endswith('.npy'):\n",
    "                try:\n",
    "                    grad_array = np.load(grad_file_path)\n",
    "                    if grad_array.shape != (W, W):  # Resize if necessary\n",
    "                        grad_array = tf.image.resize(grad_array[..., np.newaxis], (W, W)).numpy().squeeze()\n",
    "\n",
    "                    date = images_info[grad_file_name]['date']\n",
    "                    season = map_season(date)\n",
    "                    grad_images_by_season[season].append(grad_array)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {grad_file_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        for season, grad_images in grad_images_by_season.items():\n",
    "            if grad_images:\n",
    "                grad_images = np.array(grad_images)\n",
    "                mean_grad = np.mean(grad_images, axis=0)\n",
    "                mean_magnitude = np.mean(np.abs(mean_grad))\n",
    "                mean_direction = np.mean(np.sign(mean_grad))\n",
    "\n",
    "                seasonal_grad_analysis[season][cell] = {\n",
    "                    'magnitude': mean_magnitude,\n",
    "                    'direction': mean_direction,\n",
    "                    'mean_grad': mean_grad\n",
    "                }\n",
    "                print(f\"Cell: {cell}, Season: {season}, Mean Magnitude: {mean_magnitude:.4f}, Direction: {mean_direction}\")\n",
    "\n",
    "    return seasonal_grad_analysis\n",
    "\n",
    "def analyze_and_visualize_influences(cell_grad_analysis, season):\n",
    "    \"\"\"\n",
    "    Analyzes gradient influences and visualizes results.\n",
    "    \"\"\"\n",
    "    increase_cells = {cell: data for cell, data in cell_grad_analysis.items() if data[\"direction\"] > 0}\n",
    "    decrease_cells = {cell: data for cell, data in cell_grad_analysis.items() if data[\"direction\"] < 0}\n",
    "\n",
    "    increase_magnitudes = [data[\"magnitude\"] for data in increase_cells.values()]\n",
    "    decrease_magnitudes = [data[\"magnitude\"] for data in decrease_cells.values()]\n",
    "\n",
    "    increase_mean = np.mean(increase_magnitudes)\n",
    "    increase_std = np.std(increase_magnitudes)\n",
    "    increase_threshold = increase_mean + increase_std\n",
    "\n",
    "    decrease_mean = np.mean(decrease_magnitudes)\n",
    "    decrease_std = np.std(decrease_magnitudes)\n",
    "    decrease_threshold = decrease_mean + decrease_std\n",
    "\n",
    "    large_increase_cells = {cell: data[\"magnitude\"] for cell, data in increase_cells.items() if data[\"magnitude\"] > increase_threshold}\n",
    "    large_decrease_cells = {cell: data for cell, data in decrease_cells.items() if data[\"magnitude\"] > decrease_threshold}\n",
    "\n",
    "    # Visualize results\n",
    "    plot_influences(increase_cells, increase_magnitudes, increase_mean, increase_threshold, \"Increase\", season)\n",
    "    plot_influences(decrease_cells, decrease_magnitudes, decrease_mean, decrease_threshold, \"Decrease\", season)\n",
    "\n",
    "    return large_decrease_cells, decrease_magnitudes\n",
    "\n",
    "def plot_influences(cells, magnitudes, mean, threshold, label, season):\n",
    "    \"\"\"\n",
    "    Plots gradient magnitudes for the specified label (Increase/Decrease) and season.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(cells.keys(), magnitudes, label=label)\n",
    "    plt.axhline(y=mean, color=\"r\", linestyle=\"--\", label=\"Mean Magnitude\")\n",
    "    plt.axhline(y=threshold, color=\"g\", linestyle=\"--\", label=\"Large Threshold\")\n",
    "    plt.title(f\"Gradient Magnitudes for {label} Cells ({season})\")\n",
    "    plt.xlabel(\"Cells\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def save_top_25_percent_means(large_decrease_cells, river_masks_cut, season, output_folder=\"output\"):\n",
    "    \"\"\"\n",
    "    Saves the mean gradients of the top 25% negative influence cells as images and stores metadata.\n",
    "\n",
    "    Args:\n",
    "        large_decrease_cells (dict): Cells with the largest negative influence.\n",
    "        river_masks_cut (dict): River masks for each cell.\n",
    "        season (str): Current season being processed.\n",
    "        output_folder (str): Folder to save the raster outputs.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    top_25_metadata = []\n",
    "    total_cells = len(large_decrease_cells)\n",
    "    for cell, data in large_decrease_cells.items():\n",
    "        mean_grad = data['mean_grad']\n",
    "        river_mask = river_masks_cut[cell]\n",
    "        river_mean_grad = mean_grad * river_mask  # Apply river mask\n",
    "\n",
    "        # Save the raster image\n",
    "        output_path = os.path.join(output_folder, f\"{cell}_{season}_mean_grad.tif\")\n",
    "        with rasterio.open(\n",
    "            output_path,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=river_mean_grad.shape[0],\n",
    "            width=river_mean_grad.shape[1],\n",
    "            count=1,\n",
    "            dtype=river_mean_grad.dtype\n",
    "        ) as dst:\n",
    "            dst.write(river_mean_grad, 1)\n",
    "\n",
    "        # Save metadata\n",
    "        top_25_metadata.append({'cell': cell, 'season': season, 'path': output_path})\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Season: {season}, Total cells in top 25%: {len(large_decrease_cells)}/{total_cells}\")\n",
    "    return top_25_metadata\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate RMSE.\"\"\"\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def map_season(date):\n",
    "    \"\"\"Maps a date to a season based on the month.\"\"\"\n",
    "    month = pd.to_datetime(date).month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5, 9, 10, 11]:\n",
    "        return 'Spring-Autumn'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "\n",
    "def save_gradient_images(output_folder, grad_maps, season):\n",
    "    \"\"\"Saves gradient maps as images for visualization.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for variable, cells in grad_maps.items():\n",
    "        for cell, gradients in cells.items():\n",
    "            mean_grad = np.mean(gradients, axis=0)\n",
    "            output_path = os.path.join(output_folder, f\"{season}_{variable}_{cell}.png\")\n",
    "            plt.imshow(mean_grad, cmap=\"bwr\")\n",
    "            plt.title(f\"Gradient Map: {variable} ({cell}) - {season}\")\n",
    "            plt.colorbar()\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "\n",
    "def save_top_25_percent_means(large_decrease_cells, river_masks_cut, season, output_folder=\"results/top_25\"):\n",
    "    \"\"\"\n",
    "    Saves mean gradients for top 25% negative influence cells as images and stores metadata.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    top_25_metadata = []\n",
    "    total_cells = len(large_decrease_cells)\n",
    "\n",
    "    for cell, data in large_decrease_cells.items():\n",
    "        mean_grad = data['mean_grad']\n",
    "        river_mask = river_masks_cut[cell]\n",
    "        river_mean_grad = mean_grad * river_mask  # Apply river mask\n",
    "\n",
    "        # Save the raster image\n",
    "        output_path = os.path.join(output_folder, f\"{cell}_{season}_mean_grad.tif\")\n",
    "        with rasterio.open(\n",
    "            output_path,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=river_mean_grad.shape[0],\n",
    "            width=river_mean_grad.shape[1],\n",
    "            count=1,\n",
    "            dtype=river_mean_grad.dtype\n",
    "        ) as dst:\n",
    "            dst.write(river_mean_grad, 1)\n",
    "\n",
    "        # Save metadata\n",
    "        top_25_metadata.append({'cell': cell, 'season': season, 'path': output_path})\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Season: {season}, Total cells in top 25%: {len(large_decrease_cells)}/{total_cells}\")\n",
    "    return top_25_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d8243-cf42-4271-b405-4e97642b0546",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e225bf99-7eb8-4dbb-b60f-cad96e4f2e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('ERROR') \n",
    "model_name = 'simple_CNN'\n",
    "model = load_model(f'../models/{model_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8a0320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Image_Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">802,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Image_Input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m802,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">826,241</span> (3.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m826,241\u001b[0m (3.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">826,241</span> (3.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m826,241\u001b[0m (3.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_file = f\"../models/{model_name}_summary.txt\"\n",
    "with open(summary_file, \"w\") as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a3b37",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3349b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "inputs = ['lst', 'ndvi','slope','altitude','direction']\n",
    "W = 128\n",
    "var_channels = {'lst': 0, 'ndvi': 1, 'slope': 2, 'altitude': 3, 'direction': 4}\n",
    "var_position = {'month': [0, 1], 'coords': [2, 3], 'discharge': 4}\n",
    "\n",
    "for split in range(1, 6):\n",
    "    data_folder = f'../data/processed_data/{W}x{W}/{split}'\n",
    "    test_model_input, test_additional_inputs, test_target = load_set(data_folder, inputs, 'test', var_channels, var_position)\n",
    "    datasets[f'split_{split}'] = [test_model_input, test_additional_inputs, test_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b24dd0",
   "metadata": {},
   "source": [
    "## Load river files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a03cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_cells = {}\n",
    "output_directory = '../data/external/shp/river_cells_oficial'\n",
    "a=0\n",
    "\n",
    "for i,riv in enumerate(os.listdir(output_directory)):\n",
    "    # Check if the file is a shapefile\n",
    "    try:\n",
    "        if riv.endswith('.shp'):\n",
    "            a+=1\n",
    "            print(f\"Processing file: {riv}\")\n",
    "            river = gpd.read_file(os.path.join(output_directory, riv))\n",
    "            river = river.to_crs(\"EPSG:4326\")\n",
    "            river_cells[riv.split('station_')[-1].split('.')[0]] = river\n",
    "            print('Added', riv.split('station_')[-1].split('.')[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {riv}\",e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cc0a3",
   "metadata": {},
   "source": [
    "## Load raster files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = '../data/external/raster_masks/'\n",
    "files = os.listdir(dir_path)\n",
    "river_masks = {}\n",
    "\n",
    "for raster in files:\n",
    "    cc = raster.split('bw_')[-1].split('.')[0]\n",
    "    raster_path = os.path.join(dir_path, raster)\n",
    "    rast, _ = load_raster(raster_path, False)\n",
    "    river_masks[cc] = rast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb068616",
   "metadata": {},
   "source": [
    "# Model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ba7bd",
   "metadata": {},
   "source": [
    "### Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disch_file = '../data/raw/disch/discharge.csv'\n",
    "disch_df = pd.read_csv(disch_file)\n",
    "\n",
    "labels_ordered = {}\n",
    "for k, v in datasets.items():\n",
    "    labels = []\n",
    "    additional_inputs = v[1]\n",
    "    disch_vector = additional_inputs[4]  # var_position = {'month': [0, 1], 'coords': [2, 3], 'discharge': 4}\n",
    "\n",
    "    for disch in disch_vector:\n",
    "        # Buscar en todas las columnas del DataFrame si el valor `disch` está presente\n",
    "        for column in disch_df.columns:\n",
    "            if disch in disch_df[column].values:\n",
    "                labels.append(column)\n",
    "    \n",
    "    labels_ordered[k] = labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476e16c",
   "metadata": {},
   "source": [
    "### Results obtention pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline\n",
    "results_folder = \"results\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "metrics_by_splits = {}\n",
    "\n",
    "for k, v in datasets.items():\n",
    "    print('Doing split ', k)\n",
    "    test_input = v[0]\n",
    "    additional_inputs_test = v[1]\n",
    "    test_target = v[2]\n",
    "\n",
    "    y_pred = model.predict(test_input)\n",
    "\n",
    "    y_test = np.array(test_target).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    metrics_by_splits[k] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse}\n",
    "\n",
    "    # Prepare data for gradient analysis\n",
    "    test_inp_labels = {}\n",
    "    test_dates = {}\n",
    "    test_additional_inp = {}\n",
    "\n",
    "    for i, l in enumerate(labels_ordered[k]):\n",
    "        if l not in test_inp_labels:\n",
    "            test_inp_labels[l] = [test_input[i]]\n",
    "            test_additional_inp[l] = [additional_inputs_test[i]]\n",
    "        else:\n",
    "            test_inp_labels[l].append(test_input[i])\n",
    "            test_additional_inp[l].append(additional_inputs_test[i])\n",
    "\n",
    "    # Analyze gradients\n",
    "    grad_maps, variable_impact_general, variable_impact_river, cell_grad_analysis, river_grad_analysis = analyze_gradients(\n",
    "        model=model,\n",
    "        test_inp_labels=test_inp_labels,\n",
    "        test_additional_inp=test_additional_inp,\n",
    "        river_masks_cut=river_masks\n",
    "    )\n",
    "\n",
    "    # Analyze by season\n",
    "    seasonal_grad_analysis = analyze_gradients_by_season(\n",
    "        grad_folder=\"../plots/grad_cam/simple_CNN\",\n",
    "        test_inp_labels=test_inp_labels,\n",
    "        river_masks_cut=river_masks,\n",
    "        W=64\n",
    "    )\n",
    "\n",
    "    # Save gradient images\n",
    "    save_gradient_images(output_folder=os.path.join(results_folder, f\"gradients_{k}\"), grad_maps=grad_maps, season=k)\n",
    "\n",
    "    # Analyze and visualize influences\n",
    "    for season, season_data in seasonal_grad_analysis.items():\n",
    "        increase_cells, decrease_cells = analyze_and_visualize_influences(cell_grad_analysis=season_data, season=season)\n",
    "\n",
    "        # Filter and save top 25% of cells with negative influence\n",
    "        top_25_metadata = save_top_25_percent_means(\n",
    "            large_decrease_cells=decrease_cells,\n",
    "            river_masks_cut=river_masks,\n",
    "            season=season,\n",
    "            output_folder=os.path.join(results_folder, \"top_25\")\n",
    "        )\n",
    "\n",
    "        # Save metadata\n",
    "        with open(os.path.join(results_folder, f\"{season}_top_25_metadata.txt\"), \"w\") as f:\n",
    "            for item in top_25_metadata:\n",
    "                f.write(f\"Cell: {item['cell']}, Season: {item['season']}, Path: {item['path']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd5834",
   "metadata": {},
   "source": [
    "# Download true color photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = []\n",
    "\n",
    "for date in unique_dates:\n",
    "  year = int(date.split('-')[0])\n",
    "  month = int(date.split('-')[1])\n",
    "  fd = first_day(month, year)\n",
    "  ld = last_day(fd)\n",
    "  slots.append( (fd.isoformat(), ld.isoformat()) )\n",
    "\n",
    "slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d77569",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript = \"\"\"\n",
    "    //VERSION=3\n",
    "\n",
    "    let minVal = 0.0;\n",
    "    let maxVal = 0.4;\n",
    "\n",
    "    let viz = new DefaultVisualizer(minVal, maxVal);\n",
    "\n",
    "    function evaluatePixel(samples) {\n",
    "        let val = [samples.B04, samples.B03, samples.B02];\n",
    "        val = viz.processList(val);\n",
    "        val.push(samples.dataMask);\n",
    "        return val;\n",
    "    }\n",
    "\n",
    "    function setup() {\n",
    "    return {\n",
    "        input: [{\n",
    "        bands: [\n",
    "            \"B02\",\n",
    "            \"B03\",\n",
    "            \"B04\",\n",
    "            \"dataMask\"\n",
    "        ]\n",
    "        }],\n",
    "        output: {\n",
    "        bands: 4\n",
    "        }\n",
    "    }\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = [('2013-06-01', '2013-06-30')]\n",
    "river = rivers['cell_163']\n",
    "get_data(river, evalscript, slots, 'lst', destination_folder)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
