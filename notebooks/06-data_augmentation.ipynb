{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26413e9b-2434-46ad-8e6c-38fa5686dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pickle\n",
    "import time\n",
    "from contextlib import redirect_stdout\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6413a-559f-482e-8524-d411f7b8bc78",
   "metadata": {},
   "source": [
    "# Load and augment prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "851839b0-08f2-4e4b-ab1b-6e632f108290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def augment_data(idx, inputs_label, reference, outlier_mask, W, additional_inputs=False, visualize=False, augment_iterations=1): \n",
    "    \"\"\"\n",
    "    Augments the data by applying transformations to the input images. Allows for multiple augmentation iterations.\n",
    "\n",
    "    Args:\n",
    "        idx (int): Index of the data point.\n",
    "        inputs_label (tuple): Tuple containing the image and label (and additional inputs if applicable).\n",
    "        reference (tuple): Reference data containing months and coordinates.\n",
    "        outlier_mask (dict): Dictionary mapping (month, coords, label) to outlier status.\n",
    "        W (int): Image width/height.\n",
    "        additional_inputs (bool): Whether additional inputs are included.\n",
    "        visualize (bool): Whether to return augmented outputs for visualization.\n",
    "        augment_iterations (int): Number of augmentations to apply per image.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset containing augmented images and labels.\n",
    "    \"\"\"\n",
    "    if additional_inputs:\n",
    "        image = tf.cast(inputs_label[0][0], dtype=tf.float32)\n",
    "        additional_inp = tf.cast(inputs_label[0][1], dtype=tf.float32)\n",
    "        label = tf.cast(inputs_label[1], dtype=tf.float32)\n",
    "    else:\n",
    "        image = tf.cast(inputs_label[0], dtype=tf.float32)\n",
    "        label = tf.cast(inputs_label[1], dtype=tf.float32)\n",
    "    \n",
    "    reference = (tf.cast(reference[0], dtype=tf.float32), tf.cast(reference[1], dtype=tf.float32))\n",
    "    reference_month = tf.gather(reference[0], idx)\n",
    "    reference_coords = tf.gather(reference[1], idx)\n",
    "\n",
    "    reference_month = tf.expand_dims(reference_month, axis=0)  # Convert to [1]\n",
    "    label = tf.expand_dims(label, axis=0) \n",
    "\n",
    "    def get_outlier_status(month, coords, lbl):\n",
    "        python_key = (float(month), tuple(map(float, coords)), float(lbl))\n",
    "        return outlier_mask.get(python_key, False)\n",
    "    \n",
    "    is_outlier = tf.py_function(\n",
    "        func=lambda month, coords, lbl: get_outlier_status(month, coords, lbl),\n",
    "        inp=[reference_month, reference_coords, label],\n",
    "        Tout=tf.bool\n",
    "    )\n",
    "\n",
    "    # Generate base rotations\n",
    "    base_rotations = [\n",
    "        image,  # 0°\n",
    "        tf.image.rot90(image, k=1),  # 90°\n",
    "        tf.image.rot90(image, k=2),  # 180°\n",
    "        tf.image.rot90(image, k=3),  # 270°\n",
    "    ]\n",
    "\n",
    "    base_mirrors = [\n",
    "        tf.image.flip_left_right(image),  # Mirror left-to-right\n",
    "        tf.image.flip_up_down(image),    # Mirror top-to-bottom\n",
    "    ]\n",
    "\n",
    "    all_base_transformations = base_rotations + base_mirrors\n",
    "    \n",
    "    def augment(transformations, iterations):\n",
    "        \"\"\"Generates multiple augmentations by repeating the transformations.\"\"\"\n",
    "        return transformations * iterations\n",
    "\n",
    "    # Apply augmentations based on outlier status\n",
    "    \n",
    "    transformations = tf.cond(\n",
    "        is_outlier,\n",
    "        lambda: augment(all_base_transformations, max(2, augment_iterations * 2)),\n",
    "        lambda: augment(all_base_transformations, augment_iterations) +  [tf.zeros_like(all_base_transformations[0]) for _ in range(len(all_base_transformations)*augment_iterations)]#[tf.zeros_like(all_base_transformations[0]) for _ in range(len(all_base_transformations*augment_iterations*2)-len(all_base_transformations))]#augment(all_base_transformations, 0) + [tf.zeros_like(all_base_transformations[0]) for _ in all_base_transformations]\n",
    "    )\n",
    "\n",
    "\n",
    "    valid_mask = tf.logical_not(tf.reduce_all(tf.equal(transformations, 0), axis=[1, 2, 3]))\n",
    "    transformations = tf.boolean_mask(transformations, valid_mask)\n",
    "    num_channels = tf.get_static_value(tf.shape(image)[-1])\n",
    "    transformations = tf.map_fn(\n",
    "                    lambda trans: tf.ensure_shape(trans, [W, W, num_channels]),\n",
    "                    transformations\n",
    "                )\n",
    "\n",
    "    def pack_output(trans):\n",
    "        if additional_inputs:\n",
    "            return idx, (trans, tf.convert_to_tensor(additional_inp)), label\n",
    "        else:\n",
    "            return idx, trans, label\n",
    "\n",
    "    if visualize:\n",
    "        visualize_augmented_outputs = [\n",
    "            {\"image\": trans, \"label\": label, \"is_outlier\": is_outlier} for trans in transformations\n",
    "        ]\n",
    "        return visualize_augmented_outputs\n",
    "        \n",
    "    augmented_dataset = tf.data.Dataset.from_tensor_slices(transformations).map(\n",
    "        lambda trans: pack_output(trans),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "def prepare_dataset(ds, target, conditioned, W=128, reference=None, augment=False, augment_iterations=1):\n",
    "    \"\"\"\n",
    "    Prepares the dataset with optional augmentation.\n",
    "\n",
    "    Args:\n",
    "        ds (tuple): Tuple of input data.\n",
    "        target (array): Target data.\n",
    "        conditioned (bool): Whether additional inputs are included.\n",
    "        W (int): Image width/height.\n",
    "        reference (tuple): Reference data containing months and coordinates.\n",
    "        augment (bool): Whether to apply augmentation.\n",
    "        augment_iterations (int): Number of augmentations to apply per image.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Prepared dataset.\n",
    "    \"\"\"\n",
    "    inputs = (ds[0], ds[1]) if conditioned else ds\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, target))\n",
    "    \n",
    "    dataset_with_indices = dataset.enumerate() \n",
    "    \n",
    "    if augment:\n",
    "        with open('../data/external/outlier_dict.json', 'r') as json_file:\n",
    "            loaded_dict = json.load(json_file)\n",
    "        outlier_mask = {eval(key): value for key, value in loaded_dict.items()}\n",
    "\n",
    "        def augment_with_index(idx, inputs_target):\n",
    "            augmented = augment_data(\n",
    "                idx, inputs_target, reference, outlier_mask, W,\n",
    "                additional_inputs=conditioned, visualize=False, augment_iterations=augment_iterations\n",
    "            )\n",
    "            return augmented\n",
    "\n",
    "        dataset_with_indices = dataset_with_indices.flat_map(\n",
    "            augment_with_index\n",
    "        )\n",
    "    \n",
    "    return dataset_with_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4fc98f6-77d1-45ed-86f5-3647cfb1c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model input shapes: [(784, 128, 128, 5), (784, 5)]\n",
      "Número total de elementos en el dataset: 24330\n",
      "train model input shapes: [(784, 128, 128, 5), (784, 5)]\n",
      "Número total de elementos en el dataset: 24270\n",
      "train model input shapes: [(784, 128, 128, 5), (784, 5)]\n",
      "Número total de elementos en el dataset: 24330\n",
      "train model input shapes: [(784, 128, 128, 5), (784, 5)]\n",
      "Número total de elementos en el dataset: 24180\n",
      "train model input shapes: [(784, 128, 128, 5), (784, 5)]\n",
      "Número total de elementos en el dataset: 24060\n"
     ]
    }
   ],
   "source": [
    "var_channels = {'lst': 0, 'ndvi': 1, 'slope': 2, 'altitude': 3, 'direction': 4}\n",
    "var_position = {'month': [0, 1], 'coords': [2, 3], 'discharge': 4}\n",
    "with open('../data/external/cos_to_month.pkl', 'rb') as file:\n",
    "    cos_to_month = pickle.load(file)\n",
    "\n",
    "inputs = ['lst', 'ndvi','slope','altitude','direction','month','coords','discharge']\n",
    "augment = True\n",
    "W = 128\n",
    "\n",
    "train_dss = {}\n",
    "for split in range(1,6):\n",
    "    data_folder = f'../data/processed_data/{W}x{W}/{split}'\n",
    "        \n",
    "    train_model_input, train_additional_inputs, train_target = load_set(data_folder, inputs, 'train', var_channels, var_position)\n",
    "    \n",
    "    months = [cos_to_month[val] for val in train_additional_inputs[..., var_position['month'][0]]]\n",
    "    reference = (months, train_additional_inputs[..., var_position['coords']]) \n",
    "    conditioned = len(train_model_input) == 2\n",
    "    \n",
    "    train_dataset = prepare_dataset(train_model_input, train_target, conditioned, W, reference, augment=True, augment_iterations=5)\n",
    "    train_dss[split] = train_dataset\n",
    "    count = train_dataset.reduce(0, lambda x, _: x + 1)\n",
    "    print(\"Número total de elementos en el dataset:\", count.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8cf3f73-029a-4047-be36-35b930cf12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice: 0\n",
      "Forma de inputs: [((128, 128, 5), (5,))]\n",
      "Forma de target: (1,)\n"
     ]
    }
   ],
   "source": [
    "for element in train_dataset.as_numpy_iterator():\n",
    "    inputs = element[1:-1]  \n",
    "    target = element[-1] \n",
    "    idx = element[0]\n",
    "    \n",
    "    if isinstance(inputs, tuple):  # Si inputs es una tupla\n",
    "        print(\"Índice:\", idx)\n",
    "        print(\"Forma de inputs:\", [(x[0].shape, x[1].shape) for x in inputs])  # Forma de cada input en la tupla\n",
    "    else:\n",
    "        print(\"Índice:\", idx)\n",
    "        print(\"Forma de inputs:\", inputs.shape)  # Forma del input\n",
    "    print(\"Forma de target:\", target.shape)  # Forma del target\n",
    "    break  # Solo inspecciona el primer elemento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "509ecb65-b5e1-45c7-9aa6-0f2504a3f774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de elementos en el dataset: 982\n"
     ]
    }
   ],
   "source": [
    "count = train_dataset.reduce(0, lambda x, _: x + 1)\n",
    "print(\"Número total de elementos en el dataset:\", count.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8fc05-d23f-4235-8cb6-fea6a8cdb48f",
   "metadata": {},
   "source": [
    "## Analyze augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "375b98da-f21a-4bfb-bbd6-68342dc49cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_augmented_outliers(outliers, idx):\n",
    "    \"\"\"\n",
    "    Plots augmented outlier images in a grid.\n",
    "\n",
    "    Args:\n",
    "        outliers (list): List of augmented outlier dictionaries containing \"image\" keys.\n",
    "        idx (int): Index of the data point being visualized.\n",
    "    \"\"\"\n",
    "    num_samples = len(outliers)\n",
    "    rows = 3  # Number of rows\n",
    "    cols = 4  # Number of columns (adjust to match total samples)\n",
    "\n",
    "    plt.figure(figsize=(15, rows * 5))  # Adjust height proportionally to rows\n",
    "    plt.suptitle(f\"Augmented Outlier Images - Index {idx}\")\n",
    "\n",
    "    for i, outlier in enumerate(outliers):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(outlier[\"image\"][:, :, 0].numpy(), cmap=\"gray\")  # Convert to NumPy and show\n",
    "        plt.title(f\"Rotation {i}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('../plots/augmented_outliers.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5b5807a-d0ec-4cdb-858f-68ccb4d96e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n",
      "train model input shape: (784, 64, 64, 1)\n",
      "validation model input shape: (261, 64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 13:35:40.905063: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2\n",
      "train model input shape: (784, 64, 64, 1)\n",
      "validation model input shape: (261, 64, 64, 1)\n",
      "Split 3\n",
      "train model input shape: (784, 64, 64, 1)\n",
      "validation model input shape: (261, 64, 64, 1)\n",
      "Split 4\n",
      "train model input shape: (784, 64, 64, 1)\n",
      "validation model input shape: (261, 64, 64, 1)\n",
      "Split 5\n",
      "train model input shape: (784, 64, 64, 1)\n",
      "validation model input shape: (261, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "total_original = []\n",
    "outs = []\n",
    "no_outs = []\n",
    "outs_augmented = []\n",
    "no_outs_augmented = []\n",
    "total_augmented = []\n",
    "conditioned = False\n",
    "with open('../data/external/cos_to_month.pkl', 'rb') as file:\n",
    "    cos_to_month = pickle.load(file) \n",
    "\n",
    "for split in range(1,6):\n",
    "    print('Split',split)\n",
    "    W=64\n",
    "    inputs = ['lst']\n",
    "    data_folder = f'../data/processed_data/{W}x{W}/{split}'\n",
    "    var_channels = {'lst': 0, 'ndvi': 1, 'slope': 2, 'altitude': 3, 'direction': 4}\n",
    "    var_position = {'month': [0, 1], 'coords': [2, 3], 'discharge': 4}\n",
    "    train_model_input, train_additional_inputs, train_target = load_set(data_folder, inputs, 'train', var_channels, var_position)\n",
    "    val_model_input, validation_additional_inputs, validation_target = load_set(data_folder, inputs, 'validation', var_channels, var_position)\n",
    "    \n",
    "    months = [cos_to_month[val] for val in train_additional_inputs[..., var_position['month'][0]]]\n",
    "    reference = (months, train_additional_inputs[..., var_position['coords']])\n",
    "    \n",
    "    outlier_mask = np.load('../data/external/outliers_mask.npy')\n",
    "    with open('../data/external/outlier_dict.json', 'r') as json_file:\n",
    "        loaded_dict = json.load(json_file)\n",
    "    \n",
    "    outlier_mask = {eval(key): value for key, value in loaded_dict.items()}\n",
    "    \n",
    "    ds = train_model_input\n",
    "    target = train_target\n",
    "    inputs = ds\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, target))\n",
    "    dataset_with_index = dataset.enumerate()\n",
    "    show=0\n",
    "    show_no_outliers=0\n",
    "    total_outliers = 0\n",
    "    total_non_outliers = 0\n",
    "    out=0\n",
    "    no_out=0\n",
    "    total=0\n",
    "    \n",
    "    for idx, inputs in dataset.enumerate():\n",
    "        augmented_data = augment_data(idx, inputs, reference, outlier_mask, W,additional_inputs=conditioned, visualize=True,augment_iterations=1)\n",
    "    \n",
    "        # Dividir en outliers y normales\n",
    "        outliers = [x for x in augmented_data if x[\"is_outlier\"]]\n",
    "        non_outliers = [x for x in augmented_data if not x[\"is_outlier\"]]\n",
    "    \n",
    "        total_outliers += len(outliers)\n",
    "        total_non_outliers += len(non_outliers)\n",
    "    \n",
    "        # Mostrar una muestra de outliers\n",
    "        if outliers:\n",
    "            out +=1\n",
    "            if show<0:\n",
    "                print(f\"Index {idx}: Total augmentations for outlier: {len(outliers)}\")\n",
    "            \n",
    "                # Mostrar todas las augmentaciones para este outlier\n",
    "                plot_augmented_outliers(outliers, idx)\n",
    "                '''\n",
    "                plt.figure(figsize=(15, len(outliers)))\n",
    "                plt.suptitle(f\"Augmented Outlier Images - Index {idx}\")\n",
    "                \n",
    "                for i, outlier in enumerate(outliers):\n",
    "                    plt.subplot(2, 4, i + 1)\n",
    "                    plt.imshow(outlier[\"image\"][:,:,0].numpy(), cmap=\"gray\")  # Convertir a NumPy y mostrar\n",
    "                    plt.title(f\"Rotation {i}\")\n",
    "                    plt.axis(\"off\")\n",
    "                plt.savefig('../plots/augmented_outliers.png')\n",
    "                plt.show()'''\n",
    "                show+=1\n",
    "    \n",
    "        # Mostrar una muestra de no outliers\n",
    "        if non_outliers:\n",
    "            no_out+=1\n",
    "            if show_no_outliers<0:\n",
    "                print(f\"Non-Outlier Sample - Index {idx}\")\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.suptitle(\"Augmented Non-Outlier Images\")\n",
    "                for i, non_outlier in enumerate(non_outliers[:4]):  # Máximo 4 augmentaciones\n",
    "                    plt.subplot(1, 4, i + 1)\n",
    "                    plt.imshow(non_outlier[\"image\"][:,:,0].numpy(), cmap=\"gray\")\n",
    "                    plt.title(f\"Non-Outlier Rotation {i}\")\n",
    "                    plt.axis(\"off\")\n",
    "                \n",
    "                #plt.savefig('../plots/augmented_non_outliers.png')\n",
    "                plt.show()\n",
    "                show_no_outliers+=1\n",
    "        total+=1\n",
    "\n",
    "    \n",
    "        # Rompe el bucle después de visualizar un ejemplo de cada tipo\n",
    "        if outliers and non_outliers:\n",
    "            break\n",
    "            \n",
    "    total_original.append(total)\n",
    "    outs.append(out)\n",
    "    no_outs.append(no_out)\n",
    "    outs_augmented.append(total_outliers)\n",
    "    no_outs_augmented.append(total_non_outliers)\n",
    "    total_augmented.append(total_outliers + total_non_outliers)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d29cb612-9157-47ca-b171-ea034c1a9153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Total Count</th>\n",
       "      <th>Outliers Count</th>\n",
       "      <th>Non Outliers Count</th>\n",
       "      <th>Augmented Outliers Count</th>\n",
       "      <th>Augmented Non-Outliers Count</th>\n",
       "      <th>Augmented Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>324</td>\n",
       "      <td>4542</td>\n",
       "      <td>4866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>784</td>\n",
       "      <td>25</td>\n",
       "      <td>759</td>\n",
       "      <td>300</td>\n",
       "      <td>4554</td>\n",
       "      <td>4854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>324</td>\n",
       "      <td>4542</td>\n",
       "      <td>4866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784</td>\n",
       "      <td>22</td>\n",
       "      <td>762</td>\n",
       "      <td>264</td>\n",
       "      <td>4572</td>\n",
       "      <td>4836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>18</td>\n",
       "      <td>766</td>\n",
       "      <td>216</td>\n",
       "      <td>4596</td>\n",
       "      <td>4812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Total Count  Outliers Count  Non Outliers Count  \\\n",
       "0                   784              27                 757   \n",
       "1                   784              25                 759   \n",
       "2                   784              27                 757   \n",
       "3                   784              22                 762   \n",
       "4                   784              18                 766   \n",
       "\n",
       "   Augmented Outliers Count  Augmented Non-Outliers Count  \\\n",
       "0                       324                          4542   \n",
       "1                       300                          4554   \n",
       "2                       324                          4542   \n",
       "3                       264                          4572   \n",
       "4                       216                          4596   \n",
       "\n",
       "   Augmented Total Count  \n",
       "0                   4866  \n",
       "1                   4854  \n",
       "2                   4866  \n",
       "3                   4836  \n",
       "4                   4812  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Original Total Count':total_original, 'Outliers Count':outs, 'Non Outliers Count':no_outs, 'Augmented Outliers Count':outs_augmented, 'Augmented Non-Outliers Count':no_outs_augmented, 'Augmented Total Count':total_augmented}\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9554e489-cd22-47d1-afd2-c2b398155542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 1 (Parte 1):\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Original Total Count & Outliers Count & Non Outliers Count \\\\\n",
      "Split &  &  &  \\\\\n",
      "\\midrule\n",
      "0 & 784 & 27 & 757 \\\\\n",
      "1 & 784 & 25 & 759 \\\\\n",
      "2 & 784 & 27 & 757 \\\\\n",
      "3 & 784 & 22 & 762 \\\\\n",
      "4 & 784 & 18 & 766 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "Tabla 2 (Parte 2):\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Augmented Outliers Count & Augmented Non-Outliers Count & Augmented Total Count \\\\\n",
      "Split &  &  &  \\\\\n",
      "\\midrule\n",
      "0 & 648 & 9084 & 9732 \\\\\n",
      "1 & 600 & 9108 & 9708 \\\\\n",
      "2 & 648 & 9084 & 9732 \\\\\n",
      "3 & 528 & 9144 & 9672 \\\\\n",
      "4 & 432 & 9192 & 9624 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Original Total Count': total_original,\n",
    "    'Outliers Count': outs,\n",
    "    'Non Outliers Count': no_outs,\n",
    "    'Augmented Outliers Count': outs_augmented,\n",
    "    'Augmented Non-Outliers Count': no_outs_augmented,\n",
    "    'Augmented Total Count': total_augmented\n",
    "}\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.index.name = \"Split\"  # Añadir nombre al índice\n",
    "\n",
    "# Dividir el DataFrame en dos partes\n",
    "df_part1 = df[['Original Total Count', 'Outliers Count', 'Non Outliers Count']]\n",
    "df_part2 = df[['Augmented Outliers Count', 'Augmented Non-Outliers Count', 'Augmented Total Count']]\n",
    "\n",
    "# Generar el código LaTeX para ambas tablas\n",
    "latex_part1 = df_part1.to_latex(index=True)\n",
    "latex_part2 = df_part2.to_latex(index=True)\n",
    "\n",
    "# Mostrar el código LaTeX\n",
    "print(\"Tabla 1 (Parte 1):\")\n",
    "print(latex_part1)\n",
    "print(\"\\nTabla 2 (Parte 2):\")\n",
    "print(latex_part2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "552056bd-e5d5-4300-99c8-7448e5328c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Count</th>\n",
       "      <th>Outliers Count</th>\n",
       "      <th>Non Outliers Count</th>\n",
       "      <th>Augmented Outliers Count</th>\n",
       "      <th>Augmented Non-Outliers Count</th>\n",
       "      <th>Augmented Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>648</td>\n",
       "      <td>9084</td>\n",
       "      <td>9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>784</td>\n",
       "      <td>25</td>\n",
       "      <td>759</td>\n",
       "      <td>600</td>\n",
       "      <td>9108</td>\n",
       "      <td>9708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>648</td>\n",
       "      <td>9084</td>\n",
       "      <td>9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784</td>\n",
       "      <td>22</td>\n",
       "      <td>762</td>\n",
       "      <td>528</td>\n",
       "      <td>9144</td>\n",
       "      <td>9672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>18</td>\n",
       "      <td>766</td>\n",
       "      <td>432</td>\n",
       "      <td>9192</td>\n",
       "      <td>9624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Count  Outliers Count  Non Outliers Count  \\\n",
       "0             784              27                 757   \n",
       "1             784              25                 759   \n",
       "2             784              27                 757   \n",
       "3             784              22                 762   \n",
       "4             784              18                 766   \n",
       "\n",
       "   Augmented Outliers Count  Augmented Non-Outliers Count  \\\n",
       "0                       648                          9084   \n",
       "1                       600                          9108   \n",
       "2                       648                          9084   \n",
       "3                       528                          9144   \n",
       "4                       432                          9192   \n",
       "\n",
       "   Augmented Total Count  \n",
       "0                   9732  \n",
       "1                   9708  \n",
       "2                   9732  \n",
       "3                   9672  \n",
       "4                   9624  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 27*8*4\n",
    "data = {'Original Count':total_original, 'Outliers Count':outs, 'Non Outliers Count':no_outs, 'Augmented Outliers Count':outs_augmented, 'Augmented Non-Outliers Count':no_outs_augmented, 'Augmented Total Count':total_augmented}\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea4390d9-5f9b-49cb-8f62-9c46b8dea66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Count</th>\n",
       "      <th>Outliers Count</th>\n",
       "      <th>Non Outliers Count</th>\n",
       "      <th>Augmented Outliers Count</th>\n",
       "      <th>Augmented Non-Outliers Count</th>\n",
       "      <th>Augmented Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>1080</td>\n",
       "      <td>3028</td>\n",
       "      <td>4108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>784</td>\n",
       "      <td>25</td>\n",
       "      <td>759</td>\n",
       "      <td>1000</td>\n",
       "      <td>3036</td>\n",
       "      <td>4036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>1080</td>\n",
       "      <td>3028</td>\n",
       "      <td>4108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784</td>\n",
       "      <td>22</td>\n",
       "      <td>762</td>\n",
       "      <td>880</td>\n",
       "      <td>3048</td>\n",
       "      <td>3928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>18</td>\n",
       "      <td>766</td>\n",
       "      <td>720</td>\n",
       "      <td>3064</td>\n",
       "      <td>3784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Count  Outliers Count  Non Outliers Count  \\\n",
       "0             784              27                 757   \n",
       "1             784              25                 759   \n",
       "2             784              27                 757   \n",
       "3             784              22                 762   \n",
       "4             784              18                 766   \n",
       "\n",
       "   Augmented Outliers Count  Augmented Non-Outliers Count  \\\n",
       "0                      1080                          3028   \n",
       "1                      1000                          3036   \n",
       "2                      1080                          3028   \n",
       "3                       880                          3048   \n",
       "4                       720                          3064   \n",
       "\n",
       "   Augmented Total Count  \n",
       "0                   4108  \n",
       "1                   4036  \n",
       "2                   4108  \n",
       "3                   3928  \n",
       "4                   3784  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 27*8*10\n",
    "data = {'Original Count':total_original, 'Outliers Count':outs, 'Non Outliers Count':no_outs, 'Augmented Outliers Count':outs_augmented, 'Augmented Non-Outliers Count':no_outs_augmented, 'Augmented Total Count':total_augmented}\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "016b170a-3622-43ca-bc75-1f66bbad3d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Count</th>\n",
       "      <th>Outliers Count</th>\n",
       "      <th>Non Outliers Count</th>\n",
       "      <th>Augmented Outliers Count</th>\n",
       "      <th>Augmented Non-Outliers Count</th>\n",
       "      <th>Augmented Total Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>1512</td>\n",
       "      <td>3028</td>\n",
       "      <td>4540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>784</td>\n",
       "      <td>25</td>\n",
       "      <td>759</td>\n",
       "      <td>1400</td>\n",
       "      <td>3036</td>\n",
       "      <td>4436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>27</td>\n",
       "      <td>757</td>\n",
       "      <td>1512</td>\n",
       "      <td>3028</td>\n",
       "      <td>4540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784</td>\n",
       "      <td>22</td>\n",
       "      <td>762</td>\n",
       "      <td>1232</td>\n",
       "      <td>3048</td>\n",
       "      <td>4280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>18</td>\n",
       "      <td>766</td>\n",
       "      <td>1008</td>\n",
       "      <td>3064</td>\n",
       "      <td>4072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Count  Outliers Count  Non Outliers Count  \\\n",
       "0             784              27                 757   \n",
       "1             784              25                 759   \n",
       "2             784              27                 757   \n",
       "3             784              22                 762   \n",
       "4             784              18                 766   \n",
       "\n",
       "   Augmented Outliers Count  Augmented Non-Outliers Count  \\\n",
       "0                      1512                          3028   \n",
       "1                      1400                          3036   \n",
       "2                      1512                          3028   \n",
       "3                      1232                          3048   \n",
       "4                      1008                          3064   \n",
       "\n",
       "   Augmented Total Count  \n",
       "0                   4540  \n",
       "1                   4436  \n",
       "2                   4540  \n",
       "3                   4280  \n",
       "4                   4072  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Original Count':total_original, 'Outliers Count':outs, 'Non Outliers Count':no_outs, 'Augmented Outliers Count':outs_augmented, 'Augmented Non-Outliers Count':no_outs_augmented, 'Augmented Total Count':total_augmented}\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad2aeb-1a44-478c-b4a3-eff0846e56a7",
   "metadata": {},
   "source": [
    "# Save prepared augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b91628c-c0cd-4a8e-9043-5229a61461ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_as_numpy(dataset, file_path, conditioned=False):\n",
    "    indices, inputs_list, additional_inputs_list, targets = [], [], [], []\n",
    "\n",
    "    for element in dataset.as_numpy_iterator():\n",
    "        if conditioned:\n",
    "            inputs_data = element[1]  \n",
    "            inputs, additional_inputs = inputs_data\n",
    "            target = element[2] \n",
    "            index = element[0]\n",
    "        else:\n",
    "            # Correct unpacking for non-augmented datasets\n",
    "            index, (inputs_data, target) = element\n",
    "            inputs = inputs_data[0]\n",
    "            additional_inputs = None  # No additional inputs in non-augmented datasets\n",
    "\n",
    "        # Collect data\n",
    "        indices.append(index)\n",
    "        inputs_list.append(inputs)\n",
    "        if additional_inputs is not None:\n",
    "            additional_inputs_list.append(additional_inputs)\n",
    "        targets.append(target)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    indices = np.array(indices)\n",
    "    inputs = np.array(inputs_list)\n",
    "    additional_inputs = (\n",
    "        np.array(additional_inputs_list) if additional_inputs_list else None\n",
    "    )\n",
    "    targets = np.array(targets)\n",
    "    if conditioned:\n",
    "        print(len(indices), inputs[0].shape,len(additional_inputs), len(additional_inputs[0]),additional_inputs[0].shape ,targets[0].shape)\n",
    "    else:\n",
    "        print(len(indices), inputs[0].shape,targets[0].shape)\n",
    "\n",
    "    # Save to .npz\n",
    "    np.savez(\n",
    "        file_path,\n",
    "        indices=indices,\n",
    "        inputs=inputs,\n",
    "        additional_inputs=additional_inputs,\n",
    "        targets=targets,\n",
    "    )\n",
    "    print(f\"Dataset saved to {file_path}.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bad097e9-e485-4b50-8634-6a3a705c22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24330 (128, 128, 5) 24330 5 (5,) (1,)\n",
      "Dataset saved to ../data/processed_data/augmented_all_5x/1/train_dataset.npz\n",
      "24270 (128, 128, 5) 24270 5 (5,) (1,)\n",
      "Dataset saved to ../data/processed_data/augmented_all_5x/2/train_dataset.npz\n",
      "24330 (128, 128, 5) 24330 5 (5,) (1,)\n",
      "Dataset saved to ../data/processed_data/augmented_all_5x/3/train_dataset.npz\n",
      "24180 (128, 128, 5) 24180 5 (5,) (1,)\n",
      "Dataset saved to ../data/processed_data/augmented_all_5x/4/train_dataset.npz\n",
      "24060 (128, 128, 5) 24060 5 (5,) (1,)\n",
      "Dataset saved to ../data/processed_data/augmented_all_5x/5/train_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# Save training, validation, and test datasets\n",
    "dest_dir = \"../data/processed_data/augmented_all_5x\"\n",
    "os.makedirs(dest_dir, exist_ok = True)\n",
    "for split, train_ds in train_dss.items():\n",
    "    split_folder = f\"{dest_dir}/{split}\"\n",
    "    os.makedirs(split_folder, exist_ok = True)\n",
    "    save_as_numpy(train_ds, f\"{split_folder}/train_dataset\", conditioned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf691d0-744a-4720-909b-af7fe214c694",
   "metadata": {},
   "source": [
    "## Load for checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f5e6a2-e7a8-495d-ac1f-ae7dc93faa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_dataset(file_path):\n",
    "    data = np.load(file_path + \".npz\")\n",
    "    indices = data[\"indices\"]\n",
    "    inputs = data[\"inputs\"]\n",
    "    additional_inputs = data['additional_inputs']\n",
    "    print(inputs.shape,additional_inputs.shape)\n",
    "    targets = data[\"targets\"]\n",
    "    return tf.data.Dataset.from_tensor_slices((indices, ((inputs,additional_inputs), targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270543cc-7271-495d-a9e3-dbf8c84d451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3244, 128, 128, 5) (3244, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 10:21:53.076350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10530 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-12-18 10:21:53.077019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7000 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "dest_dir = \"../data/processed_data/augmented\"\n",
    "\n",
    "train_dataset = load_numpy_dataset(f\"{dest_dir}/train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8610cea8-2224-4293-a3c9-6cc12b110b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_channels(dataset, channels_to_keep):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset to keep only specified channels from the inputs.\n",
    "\n",
    "    Args:\n",
    "    - dataset: tf.data.Dataset with structure (index, ((image_inputs, additional_inputs), target)).\n",
    "    - channels_to_keep: List of channel indices to keep (e.g., [0, 2]).\n",
    "\n",
    "    Returns:\n",
    "    - A preprocessed tf.data.Dataset.\n",
    "    \"\"\"\n",
    "    def preprocess(index, inputs_targets):\n",
    "        inputs, target = inputs_targets\n",
    "        image_inputs, additional_inputs = inputs\n",
    "\n",
    "        # Select specific channels\n",
    "        selected_image_inputs = tf.gather(image_inputs, channels_to_keep, axis=-1)\n",
    "\n",
    "        # Return the new structure\n",
    "        return index, ((selected_image_inputs, additional_inputs), target)\n",
    "\n",
    "    return dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4d4990-4cdd-4f11-89dd-9cd913a7a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_to_keep = [0, 2]  # Example: Keep channels 'lst' and 'slope'\n",
    "#train_dataset = select_channels(train_dataset, channels_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5566bd-03be-41b2-b7af-f8e93b500810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Image input shape after selecting channels: (128, 128, 5)\n",
      "Additional inputs shape: (5,)\n",
      "Target shape: (1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 16:35:48.052977: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for idx, ((image_inputs, additional_inputs), target) in train_dataset.take(1):\n",
    "    print(\"Index:\", idx.numpy())\n",
    "    print(\"Image input shape after selecting channels:\", image_inputs.shape)  # Should be (128, 128, len(channels_to_keep))\n",
    "    print(\"Additional inputs shape:\", additional_inputs.shape)  # Should remain (5,)\n",
    "    print(\"Target shape:\", target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3abf8f8f-7813-4db5-b50c-47f030091048",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'RMSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed8be05-0ae6-4a61-9048-c4a930431f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation model input shapes: [(261, 128, 128, 5), (261, 5)]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "val_model_input, validation_additional_inputs, validation_target = load_set(data_folder, inputs, 'validation', var_channels, var_position)\n",
    "val_dataset = prepare_dataset(val_model_input, validation_target, conditioned, W, augment=False)\n",
    "train_dataset = train_dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "model_name = \"simple_CNN\"\n",
    "epochs = 300\n",
    "augment = True\n",
    "conditioned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ac210-f646-48cd-8019-7ce986be2dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to inspect dataset contents\n",
    "def inspect_dataset(dataset, num_samples=5):\n",
    "    print(\"Inspecting dataset...\")\n",
    "    count = 0\n",
    "    for element in dataset.take(num_samples):  # Limit to `num_samples` for inspection\n",
    "        print(len(element))\n",
    "        idx = element[0]\n",
    "        inputs = element[1]\n",
    "        inp = inputs[0][0]\n",
    "        inp2 = inputs[0][1]\n",
    "        target = inputs[1]\n",
    "\n",
    "        # Print shapes and types\n",
    "        print(f\"Sample {count + 1}:\")\n",
    "        print(f\"  Index shape: {idx.shape}, Value: {idx.numpy()}\")\n",
    "        print(f\"  Input 1 shape: {inp.shape}, Type: {inp.dtype}\")\n",
    "        print(f\"  Input 2 shape: {inp2.shape}, Type: {inp2.dtype}\")\n",
    "        print(f\"  Target shape: {target.shape}, Value: {target.numpy()}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Total samples inspected: {count}\")\n",
    "\n",
    "# Inspect train and validation datasets\n",
    "inspect_dataset(train_dataset, num_samples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f0dbe7-da68-4717-93d6-1b972af7e065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 64 128 (128, 128, 5) (128, 128, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 10:22:24.205111: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-12-18 10:22:24.205340: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for b in train_dataset.take(1):\n",
    "    print(len(b), len(b[1]), len(b[1][0]), len(b[1][0][0]),b[1][0][0].shape, b[1][0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455ecb47-bd8a-4b55-a175-6db6c6384f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2 64 (64, 128, 128, 5) (64, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 10:22:24.955198: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-12-18 10:22:24.958813: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for b in val_dataset.take(1):\n",
    "    print(len(b), len(b[1]), len(b[1][0]), len(b[1][0][0]),b[1][0][0].shape, b[1][0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964a7cf0-2c67-4700-83d6-6c201925a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 10:22:13.916711: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m conditioned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, ((image_inputs, additional_inputs), target) \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     image_inp_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(image_inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list()[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m      5\u001b[0m     additional_inp_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(additional_inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list()[\u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Start model\n",
    "conditioned = False\n",
    "for idx, ((image_inputs, additional_inputs), target) in train_dataset.take(1):\n",
    "    image_inp_shape = tuple(image_inputs.shape.as_list()[1:])\n",
    "    additional_inp_shape = tuple(additional_inputs.shape.as_list()[1:])\n",
    "    print(\"Image input shape after selecting channels:\", image_inp_shape)  # Should be (128, 128, len(channels_to_keep))\n",
    "    print(\"Additional inputs shape:\", additional_inp_shape)  # Should remain (5,)\n",
    "    print(\"Target shape:\", target.shape)\n",
    "    \n",
    "if conditioned:\n",
    "    input_args = (image_inp_shape, additional_inp_shape)\n",
    "else:\n",
    "    input_args = image_inp_shape\n",
    "    \n",
    "model = build_model_map(model_name, input_args, W)\n",
    "start_time = time.time()\n",
    "    \n",
    "summary_file = f\"../models/{model_name}_summary.txt\"\n",
    "with open(summary_file, \"w\") as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()\n",
    "\n",
    "# Set hyperparmeters variables\n",
    "#initial_lr = 0.01\n",
    "#lr_schedule = ExponentialDecay(initial_lr, decay_steps=50, decay_rate=0.96, staircase=True)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "#optimizer = tf.keras.optimizers.SGD()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "errors_log = {\"epoch\": [], \"month\": [], \"error\": []}\n",
    "loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "\n",
    "# Early Stopping parameters\n",
    "patience = 30  # Number of epochs with no improvement before stopping\n",
    "min_delta = 1e-4  # Minimum improvement required to consider progress\n",
    "best_val_loss = float('inf')  # Best observed validation loss\n",
    "wait = 0  # Counter for epochs without improvement\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  \n",
    "    num_batches = 0\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(train_model_input)).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    for batch in train_dataset:\n",
    "        if augment:\n",
    "            if conditioned:\n",
    "                model_input_batch = batch[1][0]  \n",
    "                target_batch = batch[1][1]\n",
    "                idx = batch[0]\n",
    "            else:\n",
    "                model_input_batch = batch[1][0][0] \n",
    "                target_batch = batch[1][1]\n",
    "                idx = batch[0]\n",
    "                \n",
    "        else:\n",
    "            if conditioned:\n",
    "                model_input_batch = batch[1][0]\n",
    "                target_batch = batch[1][1]  \n",
    "                idx = batch[0]\n",
    "            else:\n",
    "                idx, batch_data = batch\n",
    "                model_input_batch = batch_data[:-1]\n",
    "                target_batch = batch_data[-1]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            #y_pred = model([*model_input_batch], training=True) if conditioned else model(model_input_batch, training=True)\n",
    "            if conditioned:\n",
    "                \n",
    "                print(\"Shape of model_input_batch[0]:\", model_input_batch[0].shape)  # Images\n",
    "                print(\"Shape of model_input_batch[1]:\", model_input_batch[1].shape)  # Additional scalar inputs\n",
    "                y_pred = model([model_input_batch[0],model_input_batch[1]], training = True)\n",
    "\n",
    "            else:\n",
    "                y_pred = model(model_input_batch, training=True)\n",
    "            \n",
    "            # Compute loss based on the selected method\n",
    "            if loss_type == 'Physics_guided':\n",
    "                lst_batch = model_input_batch[0][:, :, :, :0] if conditioned else model_input_batch[:, :, :, 0]\n",
    "                loss = conservation_energy_loss(target_batch, y_pred, lst_batch, alpha=0.5, beta=0.5)\n",
    "            elif loss_type == 'RMSE_sensitive':\n",
    "                loss = rmse_extreme_sensitive(target_batch, y_pred, k1=0.01, k2=1.0, alpha=1.0)\n",
    "            elif loss_type == 'RMSE_focal':\n",
    "                loss = rmse_focal(target_batch, y_pred, gamma=1.0)\n",
    "            else:\n",
    "                loss = root_mean_squared_error(target_batch, y_pred) \n",
    "        # Calculate gradients and apply optimization\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        epoch_loss += loss.numpy()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Log variables values and error\n",
    "        y_true = tf.cast(target_batch, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        current_batch_size = y_true.shape[0]\n",
    "        batch_cosine_values = train_additional_inputs[:, 0][idx]\n",
    "    \n",
    "        # Log RMSE values for each prediction\n",
    "        for cos, pred, true in zip(batch_cosine_values, y_pred, y_true):\n",
    "            squared_error = tf.square(pred - true) \n",
    "            rmse_sample = tf.sqrt(squared_error)  # RMSE \n",
    "            rmse_value = rmse_sample.numpy()\n",
    "            errors_log[\"epoch\"].append(epoch + 1)\n",
    "            errors_log[\"month\"].append(cos_to_month[cos])\n",
    "            errors_log[\"error\"].append(rmse_value)\n",
    "                \n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    loss_per_epoch.append(avg_epoch_loss)\n",
    "    \n",
    "    # Validation loss\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for val_batch in val_dataset:\n",
    "        if conditioned:\n",
    "            val_input_batch = val_batch[1][0]\n",
    "            val_target_batch = val_batch[1][1]  \n",
    "            idx = val_batch[0]\n",
    "        else:\n",
    "            val_input_batch = val_batch[1][0][0] \n",
    "            val_target_batch = val_batch[1][1]  \n",
    "            idx = val_batch[0]\n",
    "            \n",
    "        if conditioned:\n",
    "            print(\"Shape of model_input_batch[0]:\", val_input_batch[0].shape)  # Images\n",
    "            print(\"Shape of model_input_batch[1]:\", val_input_batch[1].shape)  # Additional scalar inputs\n",
    "            val_pred = model([val_input_batch[0],val_input_batch[1]], training = False)\n",
    "\n",
    "        else:\n",
    "            val_pred = model(val_input_batch, training=False)\n",
    "            \n",
    "        #val_pred = model([*val_input_batch], training=False) if conditioned else model(val_input_batch, training=False)\n",
    "        val_loss += root_mean_squared_error(val_target_batch, val_pred).numpy()\n",
    "        val_batches += 1\n",
    "        \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    val_loss_per_epoch.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_epoch_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        # Update the best validation loss and reset patience counter\n",
    "        best_val_loss = avg_val_loss\n",
    "        wait = 0  \n",
    "        print(f\"Validation loss improved to {best_val_loss:.4f}.\")\n",
    "    else:\n",
    "        # Increment patience counter\n",
    "        wait += 1\n",
    "        print(f\"No improvement in validation loss for {wait} epochs.\")\n",
    "        if wait >= patience:\n",
    "            # Stop training if patience threshold is exceeded\n",
    "            print(f\"Stopping early at epoch {epoch + 1}.\")\n",
    "            break\n",
    "    \n",
    "    gc.collect()  # Free up memory after each epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c84742-43bc-49ad-8847-b7809da2dd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
